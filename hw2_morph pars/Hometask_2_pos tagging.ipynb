{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f155b6c",
   "metadata": {},
   "source": [
    "# Сравнение качества морфологических теггеров"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5469a657",
   "metadata": {},
   "source": [
    "*Прошу прощения за просрочку даже делайна +2 дня*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a5ba57",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "\n",
    "1. (1 балл) Создание, разметка корпуса и объяснение того, почему этот текст подходит для оценки (какие моменты вы тут считаете трудными для автоматического посттеггинга и почему, в этом вам может помочь второй ридинг). Не забывайте, что разные теггеры могут использовать разные тегсеты: напишите комментарий о том, какой тегсет вы берёте для разметки и почему.\n",
    "2. (3 балла) Потом вам будет нужно взять три POS теггера для русского языка (udpipe, stanza, natasha, pymorphy, mystem, spacy, deeppavlov) и «прогнать» текст через каждый из них.\n",
    "3. (2 балла) Затем оценим accuracy для каждого теггера. Заметьте, что в разных системах имена тегов и части речи могут отличаться, – вам надо будет свести это всё к единому стандарту с помощью какой-то функции-конвертера и сравнить с вашим размеченным руками эталоном - тоже с помощью какого-то кода или функции.\n",
    "4. (2 балла) Дальше вам нужно взять лучший теггер для русского языка и с его помощью написать функцию (chunker), которая выделяет из размеченного текста 3 типа n-грамм, соответствующих какому-то шаблону (к примеру не + какая-то часть речи или NP или сущ.+ наречие и тд)\n",
    "5. (2-3 балла) В предыдущем дз многие из вас справедливо заметили, что если бы мы могли класть в словарь не только отдельные слова, но и словосочетания, то программа работала бы лучше. Предложите 3 шаблона (слово + POS-тег / POS-тег + POS-тег) запись которых в словарь, по вашему мнению, улучшила бы качество работы программы из предыдущей домашки. Балл за объяснение того, почему именно эти группы вы взяли, балл за встраивание функции в программу из предыдущей домашки, балл за сравнение качества предсказания тональности с улучшением и без (это бонусный одиннадцатый балл)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2954ed",
   "metadata": {},
   "source": [
    "## 1. Описание данных\n",
    "\n",
    "В качестве корпуса был взят фрагмент из сказки Сергея Козлова \"Трям! Здравствуйте!\". \n",
    "\n",
    "Трудные моменты для автоматического постеггинга:\n",
    "- Новые слова: например, \"Тили-мили-трямдия\". Скорее всего в словаре теггеров нет такого слова, а значит, чтобы установить его часть речи необходимо прибегнуть к subword segmentation, что делают не все теггеры\n",
    "- Слова с дефисами: тот же пример, если перед постеггингом токенизатор разобьет такое слово на три, это будет неверно.\n",
    "- Неоднозначная нотация некоторых частей речи: например, порядковые числительные -- прилагательное или числительное? Зависит от решения создателей постеггинга. Имена персонажей -- сущ или имя собственное?\n",
    "\n",
    "Для разметки я выбрала тегсет UD, поскольку я писала на втором курсе курсач по синтагрусу в разметке UD, и просто лучше других её знаю. А ещё тегсет UD менее подробный, чем, скажем, pymorphy (OpenCorpora), поэтому свести второе к первому более-менее однозначно можно, а вот первое ко второму уже много сложнее, надо контекстные правила подключать."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bef47f",
   "metadata": {},
   "source": [
    "**Выбранные POS теггеры и объяснения, почему они**\n",
    "- stanza -- использует корпуса UD, но гораздо проще работать через питон\n",
    "- pymorphy -- тоже легко работается через питон, работает на другом корпусе (OpenCorpora), интересно сравнить\n",
    "- natasha -- разметка в формате UD, но принцип работы совсем другой, на вручную составленных правилах, хотя использует внутри себя pymorphy (не очень понимаю, как), интересно сравнить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6af84e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text_pure.txt\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b369eead",
   "metadata": {},
   "source": [
    "## 2. POS теггинг"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b402cc",
   "metadata": {},
   "source": [
    "### stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "221a5d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52ccecd41aa4701887165a3e59a4009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-06 02:58:03 INFO: Downloading default packages for language: ru (Russian)...\n",
      "2021-10-06 02:58:04 INFO: File exists: /Users/tasia/stanza_resources/ru/default.zip.\n",
      "2021-10-06 02:58:08 INFO: Finished downloading models and saved to /Users/tasia/stanza_resources.\n",
      "2021-10-06 02:58:08 INFO: Loading these models for language: ru (Russian):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "| pos       | syntagrus |\n",
      "=========================\n",
      "\n",
      "2021-10-06 02:58:08 INFO: Use device: cpu\n",
      "2021-10-06 02:58:08 INFO: Loading: tokenize\n",
      "2021-10-06 02:58:08 INFO: Loading: pos\n",
      "2021-10-06 02:58:08 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('ru')\n",
    "\n",
    "nlp = stanza.Pipeline(lang='ru', processors='tokenize,pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e8b7f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "tags_stanza = []\n",
    "for sent in doc.sentences:\n",
    "    for word in sent.words:\n",
    "        tags_stanza.append([word.text, word.upos])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e69b45",
   "metadata": {},
   "source": [
    "### pymorphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "334e67f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1f003694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OC tag: UD tag\n",
    "converter = {\n",
    "    'NOUN': 'NOUN', # не различает имена собственные\n",
    "    'ADJF': 'ADJ',\n",
    "    'ADJS': 'ADJ',\n",
    "    'COMP': 'ADV',\n",
    "    'VERB': 'VERB', # не различает вспомогательные глаголы\n",
    "    'INFN': 'VERB',\n",
    "    'PRTF': 'VERB', # сознательно будем ошибаться везде, где причастие в атрибутивной функции\n",
    "    'PRTS': 'VERB',\n",
    "    'GRND': 'VERB',\n",
    "    'NUMR': 'NUM',\n",
    "    'ADVB': 'ADV',\n",
    "    'NPRO': 'PRON', # не различает DET\n",
    "    'PRED': 'ADV', # ?\n",
    "    'PREP': 'ADP',\n",
    "    'CONJ': 'CCONJ', # не различает сочинительные и подчинительные союзы\n",
    "    'PRCL': 'PART',\n",
    "    'INTJ': 'INTJ',\n",
    "    None: 'X'\n",
    "    # нет PUNCT!\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d090d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opencorpora_to_ud_ru(oc_pos_tag):\n",
    "    return converter[oc_pos_tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d827deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тут всё будет плохо, тк очень топорно токенизирую + беру только самый вероятный разбор\n",
    "text_token = word_tokenize(text)\n",
    "tags_pymorphy = [[word, opencorpora_to_ud_ru(morph.parse(word)[0].tag.POS)] for word in text_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7a5f8",
   "metadata": {},
   "source": [
    "### natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8968a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "\n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "72f51f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "429c6364",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Doc(text)\n",
    "\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "757efb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_natasha = [tuple([word.text, word.pos, (word.start, word.stop)]) for word in doc.tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41bbf39",
   "metadata": {},
   "source": [
    "## 3. Оценка accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4bb8b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text_gold.txt\", encoding='utf-8') as f:\n",
    "    tags_gold = [line.split() for line in f if '\\t' in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e3db3138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_index(tags):\n",
    "    start = 0\n",
    "    for i, word_info in enumerate(tags):\n",
    "        word = word_info[0]\n",
    "        start = text.index(word, start)\n",
    "        end = start + len(word)\n",
    "        word_info.append((start, end))\n",
    "        tags[i] = tuple(word_info)\n",
    "        start = end\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1da9a806",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_gold = add_index(tags_gold)\n",
    "tags_stanza = add_index(tags_stanza)\n",
    "tags_pymorphy = add_index(tags_pymorphy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7c04d2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of stanza: 0.8850574712643678\n",
      "Accuracy of pymorphy: 0.42758620689655175\n",
      "Accuracy of natasha: 0.9103448275862069\n"
     ]
    }
   ],
   "source": [
    "pos_taggers = {'stanza': tags_stanza, 'pymorphy': tags_pymorphy, 'natasha': tags_natasha}\n",
    "\n",
    "gold = set(tags_gold)\n",
    "num_words = len(gold)\n",
    "for name, tags in pos_taggers.items():\n",
    "    accuracy = len(gold & set(tags)) / num_words\n",
    "    print(f\"Accuracy of {name}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42969fa3",
   "metadata": {},
   "source": [
    "Лучший POS теггер: **natasha**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6570e8",
   "metadata": {},
   "source": [
    "## 4. Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "38273845",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = [('PART', 'VERB'), ('ADV', 'VERB'), ('ADJ', 'NOUN')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b45e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# поскольку уже запускала наташу выше, это можно не повторять\n",
    "\n",
    "# from natasha import (\n",
    "#     Segmenter,\n",
    "#     MorphVocab,\n",
    "    \n",
    "#     NewsEmbedding,\n",
    "#     NewsMorphTagger,\n",
    "\n",
    "#     Doc\n",
    "# )\n",
    "\n",
    "# segmenter = Segmenter()\n",
    "# morph_vocab = MorphVocab()\n",
    "\n",
    "# emb = NewsEmbedding()\n",
    "# morph_tagger = NewsMorphTagger(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "bb1c8870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(just_text, n_gram):\n",
    "    chunks = []\n",
    "    \n",
    "    doc = Doc(just_text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    \n",
    "    for i in range(len(n_gram) - 1, len(doc.tokens)):\n",
    "        n = i - len(n_gram) + 1\n",
    "        words_pos = tuple([doc.tokens[n].pos, doc.tokens[i].pos])\n",
    "        if words_pos == n_gram:\n",
    "            chunks.append(doc.tokens[n].text + \" \" + doc.tokens[i].text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "79e8bbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Введите текст, из которого вы желаете извлечь n-граммы типа выше: Я так сильно люблю Санкт-Петербург. И не люблю Москву. Это очень шумный город. А ещё в Питере есть красивое море.\n",
      "('PART', 'VERB') ['не люблю']\n",
      "('ADV', 'VERB') ['сильно люблю']\n",
      "('ADJ', 'NOUN') ['шумный город', 'красивое море']\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Введите текст, из которого вы желаете извлечь n-граммы типа выше: \")\n",
    "for n_gram in n_grams:\n",
    "    print(n_gram, chunker(text, n_gram))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b98161",
   "metadata": {},
   "source": [
    "## 5. Улучшения к дз1\n",
    "\n",
    "Шаблоны для улучшения работы анализатора тональности (общая идея -- провести минимальный поверхностный синтаксис для установления связей с помощью синтаксических шаблонов):\n",
    "- PART + VERB -- так ловится большинство отрицаний, \"не понравился\", \"не рекомендую\" и т.д.\n",
    "- ADV + VERB -- так ловятся глаголы и какие-то тонально окрашенные наречия, \"ужасно отстирывает\", \"очень понравился\" и т.д.\n",
    "- ADJ + NOUN -- так ловятся кусочки именных группы с прилагательным, \"хороший продукт\", \"плохой кондиционер\" и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93acc0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
